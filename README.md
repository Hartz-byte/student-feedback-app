# Student Feedback Analysis Application

A full-stack application for collecting and analyzing student feedback using FastAPI backend, Streamlit frontend, and local LLM (Language Model) for summarization.

## Features

- **Feedback Collection**: Students can submit feedback with name, email, course, rating, tags, and comments
- **Feedback Management**: View, update, and delete feedback entries
- **AI-Powered Summarization**: Uses local LLM (Mistral 7B) to generate concise summaries of feedback
- **Responsive Web Interface**: Built with Streamlit for an intuitive user experience
- **RESTful API**: FastAPI backend with proper error handling and data validation

## Tech Stack

### Backend
- **Framework**: FastAPI
- **Database**: JSON file-based storage (for simplicity)
- **LLM**: Mistral 7B (local model)
- **API Documentation**: Auto-generated by FastAPI (available at `/docs` when running)

### Frontend
- **Framework**: Streamlit
- **UI Components**: Built-in Streamlit widgets
- **API Client**: Python requests library

## Prerequisites

- Python 3.10+
- pip (Python package manager)
- Local LLM model (Mistral 7B) - path should be set in the code

## Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/student-feedback-app.git
   cd student-feedback-app
   ```

2. Create and activate a virtual environment:
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: .\venv\Scripts\activate
   ```

3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

4. Install llama-cpp-python (for local LLM):
   ```bash
   pip install --upgrade --force-reinstall --no-cache-dir --verbose llama-cpp-python
   ```

## Project Structure

```
student-feedback-app/
├── backend/
│   ├── main.py          # FastAPI application
│   ├── models.py        # Pydantic models
│   └── storage.py       # Data storage utilities
├── frontend/
│   └── app.py           # Streamlit frontend
├── requirements.txt     # Python dependencies
└── README.md           # This file
```

## Running the Application

### 1. Start the Backend Server

In one terminal, run:
```bash
uvicorn backend.main:app --reload --host 0.0.0.0 --port 8000
```

The API will be available at `http://localhost:8000`

### 2. Start the Frontend

In another terminal, run:
```bash
streamlit run frontend/app.py
```

The Streamlit app will open in your default web browser at `http://localhost:8501`

## API Endpoints

- `GET /feedback` - List all feedback entries
- `POST /feedback` - Create a new feedback entry
- `PUT /feedback/{id}` - Update a feedback entry
- `DELETE /feedback/{id}` - Delete a feedback entry
- `GET /summarize/{id}` - Generate AI summary for a feedback entry

## Configuration

Make sure to set the correct path to your local LLM model in `backend/main.py`:
```python
MODEL_PATH = "path/to/your/model.gguf"
```

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request